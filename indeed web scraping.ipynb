{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6d4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc35ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the data thats getting data from the indeed.com\n",
    "\n",
    "def extract(page):\n",
    "    headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:79.0) Gecko/20100101 Firefox/79.0'}\n",
    "# my user agent can be get when you google it with the same. \n",
    "    url=f\"https://www.indeed.co.in/jobs?q=data+scientist&l=India&start={page}\"\n",
    "# start = page thats the number of pages of the search of the job we searched  \n",
    "    reg = requests.get(url,headers)\n",
    "    soup=BeautifulSoup(reg.content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed5033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the data \n",
    " \n",
    "def transform(soup):\n",
    "    divs = soup.find_all('div', class_ = 'jobsearch-SerpJobCard')\n",
    "# div class in the inspect page of the website each div class is a seperate\n",
    "# job seach that we gave in the search criteria of the website\n",
    "    #return len(divs)\n",
    "    for item in divs:\n",
    "        title = item.find('a').text.strip()\n",
    "# 1st 'a' tag within the div if we give its gives a list, and strip() removes the white spaces\n",
    "#print(title)\n",
    "#return\n",
    "        company = item.find('span',class_='company').text.strip()\n",
    "#print(company)\n",
    "        try:\n",
    "            salary = item.find('span',class_='salaryText').text.strip()\n",
    "        except:\n",
    "            salary = ''\n",
    "        summary = item.find('div',class_='summary').text.strip()\n",
    "        #required = item.find('div',class_='jobCardReqItem').text.strip()\n",
    "        location = item.find('div',class_='location accessible')\n",
    "        #urgent = item.find('tr',class_='jobCardShelfItem')\n",
    "\n",
    "        job = {\n",
    "            'title': title,\n",
    "            'company': company,\n",
    "            'salary': salary,\n",
    "            'summary': summary,\n",
    "            #'required': required,\n",
    "            'location': location\n",
    "            #'urgent': urgent\n",
    "        }\n",
    "        joblist.append(job)\n",
    "    return\n",
    "\n",
    "joblist = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e8620d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page,0\n",
      "Getting page,1\n",
      "Getting page,2\n",
      "Getting page,3\n",
      "Getting page,4\n",
      "Getting page,5\n",
      "Getting page,6\n",
      "Getting page,7\n",
      "Getting page,8\n",
      "Getting page,9\n",
      "Getting page,10\n",
      "Getting page,11\n",
      "Getting page,12\n",
      "Getting page,13\n",
      "Getting page,14\n",
      "Getting page,15\n",
      "Getting page,16\n",
      "Getting page,17\n",
      "Getting page,18\n",
      "Getting page,19\n",
      "Getting page,20\n",
      "Getting page,21\n",
      "Getting page,22\n",
      "Getting page,23\n",
      "Getting page,24\n",
      "Getting page,25\n",
      "Getting page,26\n",
      "Getting page,27\n",
      "Getting page,28\n",
      "Getting page,29\n",
      "Getting page,30\n",
      "Getting page,31\n",
      "Getting page,32\n",
      "Getting page,33\n",
      "Getting page,34\n",
      "Getting page,35\n",
      "Getting page,36\n",
      "Getting page,37\n",
      "Getting page,38\n",
      "Getting page,39\n",
      "Getting page,40\n",
      "Getting page,41\n",
      "Getting page,42\n",
      "Getting page,43\n",
      "Getting page,44\n",
      "Getting page,45\n",
      "Getting page,46\n",
      "Getting page,47\n",
      "Getting page,48\n",
      "Getting page,49\n",
      "Getting page,50\n",
      "Getting page,51\n",
      "Getting page,52\n",
      "Getting page,53\n",
      "Getting page,54\n",
      "Getting page,55\n",
      "Getting page,56\n",
      "Getting page,57\n",
      "Getting page,58\n",
      "Getting page,59\n",
      "Getting page,60\n",
      "Getting page,61\n",
      "Getting page,62\n",
      "Getting page,63\n",
      "Getting page,64\n",
      "Getting page,65\n",
      "Getting page,66\n",
      "Getting page,67\n",
      "Getting page,68\n",
      "Getting page,69\n",
      "Getting page,70\n",
      "Getting page,71\n",
      "Getting page,72\n",
      "Getting page,73\n",
      "Getting page,74\n",
      "Getting page,75\n",
      "Getting page,76\n",
      "Getting page,77\n",
      "Getting page,78\n",
      "Getting page,79\n",
      "Getting page,80\n",
      "Getting page,81\n",
      "Getting page,82\n",
      "Getting page,83\n",
      "Getting page,84\n",
      "Getting page,85\n",
      "Getting page,86\n",
      "Getting page,87\n",
      "Getting page,88\n",
      "Getting page,89\n",
      "Getting page,90\n",
      "Getting page,91\n",
      "Getting page,92\n",
      "Getting page,93\n",
      "Getting page,94\n",
      "Getting page,95\n",
      "Getting page,96\n",
      "Getting page,97\n",
      "Getting page,98\n",
      "Getting page,99\n",
      "0\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Acer'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0,100,1): #number of pages we want to web scrape in indeed\n",
    "    print(f'Getting page,{i}')\n",
    "    result = extract(0)\n",
    "    transform(result)\n",
    "#print(transform(result)) # 15 (i.e) divs that match \"jobsearch-serpJobCard\" in the 1st page\n",
    "print(len(joblist))\n",
    "print(joblist)\n",
    "df = pd.DataFrame(joblist)\n",
    "df.head()\n",
    "df.to_csv('indeedjobs1.csv')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e1dd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
